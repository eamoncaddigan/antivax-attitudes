---
layout: post
title: "Bootstrap analysis of anti-vaccination belief changes"
summary: Another way of looking at the antivaccination data of Horne, et al.
author: "Eamon Caddigan"
date: 2015-09-15
categories: psych R
output: html_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(cache=TRUE, echo=FALSE, warning=FALSE, message=FALSE,
                      fig.width=9, fig.align="center")
```

```{r setup_data, results="hide"}
# Required librarys and external files ----------------------------------------

library(readxl)
library(tidyr)
library(dplyr)
library(ggplot2)
library(gridExtra)

# Clean and process the data --------------------------------------------------

# Generates warnings for the Ps who didn't do day 2
suppressWarnings(expData <- read_excel("Vacc_HPHH_publicDataset.xlsx", sheet = 2))

# Exclude Ps who didn't do day 2 and failed the attention checks
expData.clean <- expData %>%
  # It's good to add a subject number so we can go back to original data
  mutate(subject_number = 1:nrow(.)) %>%
  filter(Returned == 1,
         `AttentionCheck_PostTest (if = 4 then include)` == 4,
         `AttentionChecks_Sum(include if = 4)` == 4,
         Paid_Attention == 1)

# Get all the dependent measures into a DF
questionnaireData <- expData.clean %>%
  # Pull out the columns and use consistent names
  select(subject_number,
         intervention = Condition,
         pretest.healthy = Healthy_VaxscalePretest,
         posttest.healthy = Healthy_VaxscalePosttest,
         pretest.diseases = Diseases_VaxScalePretest,
         posttest.diseases = Diseases_VaxScalePosttest,
         pretest.doctors = Doctors_VaxScalePreTest,
         posttest.doctors = Doctors_VaxScalePostTest,
         pretest.side_effects = Sideeffects_VaxScalePreTest,
         posttest.side_effects = Sideeffects_VaxScalePostTest,
         pretest.plan_to = Planto_VaxScalePreTest,
         posttest.plan_to = Planto_VaxScalePostTest) %>%
  # Reverse-code the approrpiate columns
  mutate(pretest.diseases = 7 - pretest.diseases,
         posttest.diseases = 7 - posttest.diseases,
         pretest.side_effects = 7 - pretest.side_effects,
         posttest.side_effects = 7 - posttest.side_effects) %>%
  # Tidy the data
  gather("question", "response", -subject_number, -intervention) %>%
  separate(question, c("interval", "question"), sep = "\\.") %>% 
  mutate(intervention = factor(intervention, 
                               c("Control", "Autism Correction", "Disease Risk")),
         interval = factor(interval, 
                           c("pretest", "posttest"), ordered = TRUE),
         question = factor(question, 
                           c("healthy", "diseases", "doctors", "side_effects", "plan_to"))) %>%
  # Pre- and post-test get their own columns in these analyses
  mutate(interval = paste0(interval, "_response")) %>%
  spread(interval, response)
# -----------------------------------------------------------------------------
```

## Introduction

In a [previous post]({{ site.url }}/psych/bayes/2015/09/03/antivax-attitudes/) (I don't know why I'm linking it since there are only two), I presented an analysis of data by ([Horne, Powell, Hummel & Holyoak, 2015](http://www.pnas.org/content/112/33/10321.abstract)) showing changes in antivaccination attitudes. This previous analysis used Bayesian estimation to show a credible increase in pro-vaccination attitudes following a "disease risk" intervention, but not an "autism correction" intervention.

Some of my friends offered insightful comments, and one [pointed out](https://twitter.com/johnclevenger/status/639795727439429632) that there appeared to be a failure of random assignment. Participants in the "disease risk" group happened to have lower scores on the survey and therefore had more room for improvement. This is a fair criticism, but I found that post-intervention scores alone were higher for the "disease risk" group, which addresses this problem. 

![Posteror of final score differences](https://pbs.twimg.com/media/COE2e8bUkAEeu8b.png:large)

# Bootstrapping 

Interpreting differences in parameter values isn't always straightforward, so I thought it'd be worthwhile to try a different approach. Instead fitting a generative model to the sampled data, we can use bootstrapping on the sample to estimate population parameters. [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) is conceptually simple; I feel it would have much wider adoption today had computers been around in the early days of statistics.

Here is code that uses bootstrapping to estimate the probability of each response on the pre-intervention survey (irrespective of question on intervention group assignment). The sample mean is already an unbiased estimator of the population mean, so bootstrapping isn't necessary in this first example. However, this provides a simple illustration of how the technique works: sample observations *with replacement* from the observed data, calculate a statistic on this new data, and repeat. 

```{r setup_bootstrap, dependson="setup_data", echo=TRUE}
numBootstraps <- 1e3  # Should be a big number
numObservations <- nrow(questionnaireData)
uniqueResponses <- sort(unique(questionnaireData$pretest_response))
interventionLevels <- levels(questionnaireData$intervention)

# The observed proportion of responses at each level
obsPretestResponseProbabilities <- as.numeric(table(questionnaireData$pretest_response)) / 
  numObservations
```

```{r pretest_bootstrap, dependson="setup_bootstrap", echo=TRUE}
# Bootstrap to find the probability that each response will be given to pre-test
# questions.

pretestData <- matrix(data = 0,
                      nrow = numBootstraps, 
                      ncol = length(uniqueResponses))
colnames(pretestData) <- paste(uniqueResponses)

# Run the bootstrap
for (ii in seq_len(numBootstraps)) {
  bootSamples <- sample(questionnaireData$pretest_response, 
                        numObservations, 
                        replace = TRUE)
  bootSamplesTabulated <- table(bootSamples)
  pretestData[ii, names(bootSamplesTabulated)] <- bootSamplesTabulated
}

# Convert the counts to probabilities
pretestData <- pretestData / numObservations
```

```{r pretest_plot, dependson="pretest_bootstrap", fig.width=4, fig.height=4}
pretestDF <- data_frame(response = uniqueResponses, 
                             bootstrap_prob = apply(pretestData, 2, mean),
                             bootstrap_sd = apply(pretestData, 2, sd),
                             observed_prob = obsPretestResponseProbabilities)
ggplot(pretestDF, aes(x = response)) + 
  geom_bar(aes(y = observed_prob), stat = "identity", 
           color="white", fill="skyblue") + 
  geom_point(aes(y = bootstrap_prob), size=3, color="red") + 
  geom_errorbar(aes(ymin = bootstrap_prob-bootstrap_sd/2, 
                    ymax = bootstrap_prob+bootstrap_sd/2),
                size=2, color="red", width=0) +
  scale_x_continuous(breaks = 1:length(uniqueResponses)) +
  scale_y_continuous(limits = c(0, 1)) +
  xlab("Response Level") + 
  ylab("Proportion") + 
  theme_classic()
```

As expected, the bootstrap estimates for the proportion of responses at each level almost exactly match the observed data.

## Changes in vaccination attitudes

Due to chance alone, the three groups of participants (the control group, the "autism correction" group, and the "disease risk" group) had different patterns of responses to the pre-intervention survey. To mitigate this, the code below estimates the transition probabilities from each response on the pre-intervention survey to each response on the post-intervention survey, and does so separately for each group. These are conditional probabilities, e.g., P(post-intervention rating = 4 | pre-intervention rating = 3). These conditional probabilities are then combined with the observed pre-intervention response probabilities to calculate the joint probability of each response transition (e.g., P(post-intervention rating = 4 AND pre-intervention rating = 3)). Since the prior is agnostic to subjects' group assignment, these joint probability estimates are free from any biases due to a failure of random assignment.

```{r posttest_bootstrap, dependson="setup_bootstrap", echo=TRUE}

# preintervention responses x intervention groups x bootstraps x postintervention responses
posttestData <- array(data = 0,
                      dim = c(length(uniqueResponses),
                              length(interventionLevels),
                              numBootstraps, 
                              length(uniqueResponses)),
                      dimnames = list(paste(uniqueResponses),
                                      interventionLevels,
                                      NULL,
                                      paste(uniqueResponses)))

for (pretestResponse in seq_along(uniqueResponses)) {
  for (interventionLevel in seq_along(interventionLevels)) {
    # Get the subset of data for each combination of intervention and
    # pre-intervention response level.
    questionnaireDataSubset <- filter(questionnaireData,
                                      intervention == interventionLevels[interventionLevel],
                                      pretest_response == pretestResponse)
    numObservationsSubset <- nrow(questionnaireDataSubset)
    
    # Run the bootstrap
    for (ii in seq_len(numBootstraps)) {
      bootSamples <- sample(questionnaireDataSubset$posttest_response, 
                            numObservationsSubset, 
                            replace = TRUE)
      bootSamplesTabulated <- table(bootSamples)
      posttestData[pretestResponse, 
                   interventionLevel, 
                   ii, 
                   names(bootSamplesTabulated)] <- bootSamplesTabulated
    }
    
    # Convert the counts to probabilities
    posttestData[pretestResponse, interventionLevel, , ] <- 
      posttestData[pretestResponse, interventionLevel, , ] / numObservationsSubset
  }
  
  # Convert the conditional probabilities to joint probabilities using the 
  # observed priors on each pretest response.
  posttestData[pretestResponse, , , ] <- posttestData[pretestResponse, , , ] *
    obsPretestResponseProbabilities[pretestResponse]
}
```

With the transition probabilities appropriately modeled, it's possible to test the hypothesis: **"participants in the 'disease risk' group are more likely to respond with a more-pro-vaccine attitude after the intervention than participants in the other groups."** We'll use the previously-run bootstraps to compute the probability of increased scores separately for each group.

```{r posttest_shifts, dependson="posttest_bootstrap", echo=TRUE}
posttestIncrease <- matrix(data = 0, 
                           nrow = numBootstraps,
                           ncol = length(interventionLevels))
colnames(posttestIncrease) <- interventionLevels

for (interventionLevel in seq_along(interventionLevels)) {
  for (pretestResponse in seq_along(uniqueResponses)) {
    for (posttestResponse in seq_along(uniqueResponses)) {
      if (posttestResponse > pretestResponse) {
        posttestIncrease[, interventionLevel] <- posttestIncrease[, interventionLevel] +
          posttestData[pretestResponse, interventionLevel, , posttestResponse]
      }
    }
  }
}
```

This estimates the probability that "disease risk" samples have a greater probability of making higher post-intervention responses than "autism correction" sample 

```{r posttest_stat, dependson="posttest_shifts", echo=TRUE}
sum(posttestIncrease[, which(interventionLevels == "Disease Risk")] > 
      posttestIncrease[, which(interventionLevels == "Autism Correction")]) / 
  nrow(posttestIncrease)
```

Below is a visualization of the bootstrapped distributions of the probabilities of increased post-intervention responses. 

```{r posttest_plot, dependson="posttest_shifts"}
posttestDF <- gather(as.data.frame(posttestIncrease), "intervention", "prob_increase")
ggplot(posttestDF, aes(x = prob_increase, fill = intervention)) + 
  geom_density(alpha = 0.6) + 
  ylab("Samples") + xlab("Probability of rating increase") + 
  theme_minimal()
```

## Conclusion

Bootstrapping shows that the "disease risk" group is more likely than other groups to have an increase in pro-vaccination attitudes following the intervention. This analysis collapses across the five survey questions used by Horne and colleagues, although it would be straightforward to extend this approach to estimate attitude changes separately for each question. 